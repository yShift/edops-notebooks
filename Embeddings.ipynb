{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUP0rzImRg0P"
      },
      "source": [
        "# Calculating Embeddings\n",
        "Let's look at three methods to calculate embeddings. We will consider calculating cosine similarity as an example, but generally these embeddings could be used in a number of different applications.\n",
        "\n",
        "We are considering two models for each method, but these models could potentially be replaced with any other models for calculating embeddings from HuggingFace (for example [here](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending)). We will compare `sentences_1` and `sentences_2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Er8ODL0vSYeM"
      },
      "outputs": [],
      "source": [
        "model_baai = \"BAAI/bge-small-en-v1.5\"\n",
        "model_multi = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
        "\n",
        "sentences_1 = [\n",
        "    \"Assessing the functionality of different embedding models.\",\n",
        "    \"Two approaches for creating embeddings are provided here.\"\n",
        "]\n",
        "# To demonstrate how the similarity metrics works we are taking similar sentences:\n",
        "sentences_2 = [\n",
        "    \"Evaluating the performance of various types of embeddings.\",\n",
        "    \"Here, we are presented with two methods for generating embeddings.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTn3LSExSU9H"
      },
      "source": [
        "### Method 1: Using Sentence Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhM-xLH1gqUl",
        "outputId": "f8b8e086-5b81-45b2-ade0-6d172cb265c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compare similarity calculated for BAAI embeddings:  [[0.9387083  0.83786726]\n",
            " [0.8362595  0.95773757]]\n",
            "and for multilingual embeddings:  [[0.7264205  0.06265946]\n",
            " [0.09025779 0.83387554]]\n"
          ]
        }
      ],
      "source": [
        "# TODO add all the necessary installations with pip install statements\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Define a function to compute cosine similarity between embeddings of two sets of sentences\n",
        "\n",
        "\n",
        "def compute_similarity_st(sentences_1, sentences_2, model_name):\n",
        "    # Initialize the SentenceTransformer model with the specified model name\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    # Encode the first set of sentences to get their embeddings, normalizing the results\n",
        "    embeddings_1 = model.encode(sentences_1, normalize_embeddings=True)\n",
        "\n",
        "    # Encode the second set of sentences to get their embeddings, normalizing the results\n",
        "    embeddings_2 = model.encode(sentences_2, normalize_embeddings=True)\n",
        "\n",
        "    # Compute the cosine similarity between the two sets of embeddings\n",
        "    # The @ operator performs matrix multiplication which is equivalent to cosine similarity\n",
        "    # when the embeddings are normalized.\n",
        "    return embeddings_1 @ embeddings_2.T\n",
        "\n",
        "\n",
        "# Compute the similarity matrices using the BAAI model and the multilingual model\n",
        "similarity_baai_st = compute_similarity_st(\n",
        "    sentences_1, sentences_2, model_baai)\n",
        "similarity_multi_st = compute_similarity_st(\n",
        "    sentences_1, sentences_2, model_multi)\n",
        "\n",
        "# Print out the cosine similarity matrices for comparison\n",
        "print(\"Compare similarity calculated for BAAI embeddings: \", similarity_baai_st)\n",
        "print(\"and for multilingual embeddings: \", similarity_multi_st)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_J_DDGMrpoD"
      },
      "source": [
        "### Method 2: Using LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "piXCQpsyUVkR"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.12\n",
        "!pip install sentence_transformers==2.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC1m_xbvj8hH",
        "outputId": "4d7f956c-a21e-4159-c47e-bfb869149e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compare similarity calculated for BAAI embeddings:  [[0.93870836 0.83786722]\n",
            " [0.83625949 0.95773762]]\n",
            "and for multilingual embeddings:  [[0.72642046 0.06265945]\n",
            " [0.09025782 0.83387566]]\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to compute cosine similarity using the HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "def compute_similarity_langchain(sentences_1, sentences_2, model_name):\n",
        "    # Initialize the HuggingFaceEmbeddings with the specified model and normalization\n",
        "    model = HuggingFaceEmbeddings(model_name=model_name, encode_kwargs={\n",
        "                                  'normalize_embeddings': True})\n",
        "\n",
        "    # Generate embeddings for the first set of sentences and convert to a numpy array\n",
        "    embeddings_1 = np.array(model.embed_documents(sentences_1))\n",
        "\n",
        "    # Generate embeddings for the second set of sentences and convert to a numpy array\n",
        "    embeddings_2 = np.array(model.embed_documents(sentences_2))\n",
        "\n",
        "    # Compute and return the cosine similarity matrix between the two sets of embeddings\n",
        "    return embeddings_1 @ embeddings_2.T\n",
        "\n",
        "\n",
        "# Compute the similarity matrices for the two models\n",
        "similarity_baai_langchain = compute_similarity_langchain(\n",
        "    sentences_1, sentences_2, model_baai)\n",
        "similarity_multi_langchain = compute_similarity_langchain(\n",
        "    sentences_1, sentences_2, model_multi)\n",
        "\n",
        "# Print out the cosine similarity matrices for comparison\n",
        "print(\"Compare similarity calculated for BAAI embeddings: \",\n",
        "      similarity_baai_langchain)\n",
        "print(\"and for multilingual embeddings: \", similarity_multi_langchain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8LZK_70S_ox"
      },
      "source": [
        "### Method 3: Loading model directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-o9EYzdQEdr",
        "outputId": "b32644c0-d711-42cb-d51b-91239bc7d3c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compare similarity calculated for BAAI embeddings:  tensor([[0.9387, 0.8379],\n",
            "        [0.8363, 0.9577]])\n",
            "and for multilingual embeddings:  tensor([[0.7713, 0.3157],\n",
            "        [0.3659, 0.8487]])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Define a function to compute embeddings and their cosine similarity\n",
        "\n",
        "\n",
        "def compute_similarity_direct(sentences_1, sentences_2, model_name):\n",
        "    # Initialize the tokenizer and model with the specified model name\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode the first set of sentences\n",
        "    encoded_input_1 = tokenizer(\n",
        "        sentences_1, padding=True, truncation=True, return_tensors='pt')\n",
        "    # Tokenize and encode the second set of sentences\n",
        "    encoded_input_2 = tokenizer(\n",
        "        sentences_2, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Generate embeddings for the first set of sentences without gradient calculation for efficiency\n",
        "    with torch.no_grad():\n",
        "        model_output_1 = model(**encoded_input_1)\n",
        "        # Extract the [CLS] token's embeddings as sentence embeddings\n",
        "        sentence_embeddings_1 = model_output_1[0][:, 0]\n",
        "\n",
        "    # Generate embeddings for the second set of sentences\n",
        "    with torch.no_grad():\n",
        "        model_output_2 = model(**encoded_input_2)\n",
        "        # Extract the [CLS] token's embeddings as sentence embeddings\n",
        "        sentence_embeddings_2 = model_output_2[0][:, 0]\n",
        "\n",
        "    # Normalize the embeddings to unit length\n",
        "    embeddings_1 = torch.nn.functional.normalize(\n",
        "        sentence_embeddings_1, p=2, dim=1)\n",
        "    embeddings_2 = torch.nn.functional.normalize(\n",
        "        sentence_embeddings_2, p=2, dim=1)\n",
        "\n",
        "    # Calculate and return the cosine similarity matrix\n",
        "    return embeddings_1 @ embeddings_2.T\n",
        "\n",
        "\n",
        "# Compute the similarity using the BAAI model\n",
        "similarity_baai_direct = compute_similarity_direct(\n",
        "    sentences_1, sentences_2, model_baai)\n",
        "# Compute the similarity using the multilingual model\n",
        "similarity_multi_direct = compute_similarity_direct(\n",
        "    sentences_1, sentences_2, model_multi)\n",
        "\n",
        "# Print the similarity results for comparison\n",
        "print(\"Compare similarity calculated for BAAI embeddings: \", similarity_baai_direct)\n",
        "print(\"and for multilingual embeddings: \", similarity_multi_direct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA8dS3OrPMWE"
      },
      "source": [
        "The discrepancy in the embeddings and resulting similarity scores you're observing between the first two and the third methods arises from the differences in how the embeddings are generated, even though the same underlying model (`\"sentence-transformers/distiluse-base-multilingual-cased-v2\"`) is used.\n",
        "\n",
        "In the first two functions, we're using the libraries, which are specifically designed to produce **sentence embeddings**. These libraries apply additional processing steps to the raw model outputs to produce optimized sentence-level embeddings.\n",
        "\n",
        "While when we are loading a model directly from the Hugging Face transformers library, we manually handle the model's output to generate embeddings, selecting the first token's output `([0][:, 0])` as the sentence representation, which is typically a token in BERT-like models. This approach doesn't necessarily align with how sentence transformers are designed to produce sentence embeddings.\n",
        "\n",
        "If you're looking to maintain consistency in embedding generation across different parts of your code or application, *it's crucial to use the same method for embedding generation*."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
